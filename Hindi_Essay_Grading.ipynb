{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from glob import glob\n",
    "\n",
    "datasets_folder_path = 'C:/Users/Chotu/Videos/Major/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_all_essays(folder_path):\n",
    "  #recursively find all .csv files in directory and sub-directory\n",
    "  all_csv_files = [file for path, subdir, files in os.walk(folder_path)\n",
    "                          for file in glob(os.path.join(path, \"*.csv\"))]\n",
    "  #print(all_csv_files)\n",
    "  return pd.concat([pd.read_csv(f) for f in all_csv_files], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "D = getting_all_essays(datasets_folder_path)\n",
    "y = D['domain1_score']\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       अगर\n",
       "1        अत\n",
       "2      अथवा\n",
       "3      अंदर\n",
       "4       अदि\n",
       "       ... \n",
       "299    होति\n",
       "300    होती\n",
       "301    होते\n",
       "302    होना\n",
       "303    होने\n",
       "Name: stopwords, Length: 304, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = pd.read_csv( 'C:/Users/Chotu/Videos/hindi_stopwords.csv')\n",
    "stop_words=P['stopwords']\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from string import digits \n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentenceEnders = re.compile('[।!?]')\n",
    "ge = re.compile(' ')\n",
    "wordy1 = []\n",
    "wordy2= []\n",
    "wordy3=[]\n",
    "flag = 0\n",
    "train_essay = D['essay']\n",
    "for i in range(len(train_essay)):\n",
    "    m=[]\n",
    "    if i>=0:\n",
    "        essay_v = train_essay[i]\n",
    "        essay_v =re.sub(\"\\u200b\",\"\",essay_v)\n",
    "        essay_v.strip()\n",
    "        #print(sentenceEnders.split(essay_v))\n",
    "        \n",
    "        X = sentenceEnders.split(essay_v)\n",
    "        #print(X)\n",
    "        wordy=[]\n",
    "        for sentence in X:\n",
    "            p = []\n",
    "            #print(ge.split(sentence))\n",
    "            #words = ge.split(sentence)\n",
    "            tokens = word_tokenize(sentence)\n",
    "            \n",
    "            remove_digits = str.maketrans('', '', digits) \n",
    "            list = [i.translate(remove_digits) for i in tokens] \n",
    "            \n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            words = [w.translate(table) for w in list ]\n",
    "            #print(words)\n",
    "            for word in words:\n",
    "                #print(word)\n",
    "                flag = 0\n",
    "                for stop_word in stop_words:\n",
    "                    #print(stop_word)\n",
    "                    #print(\"word \"+word)\n",
    "                    #print(\"-----///\")\n",
    "                    if word == stop_word or word == '':\n",
    "                        flag = 1\n",
    "                        #print(word+\" \"+stop_word)\n",
    "                if flag == 0:\n",
    "                    #print(word)\n",
    "                    wordy.append(word)\n",
    "                    p.append(word)\n",
    "            if len(p)>1:\n",
    "                wordy2.append(p)\n",
    "                m.append(p)\n",
    "            \n",
    "            \"\"\"print(\"Before\\n\")\n",
    "            #print(words)\n",
    "            print(\"\\n------\\n\")\n",
    "            print(\"After\\n\")\n",
    "            #print(wordy)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            \"\"\"\n",
    "    \n",
    "        wordy1.append(wordy)\n",
    "        \n",
    "        #print(\"-----\")\n",
    "    wordy3.append(m)\n",
    "    #print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wordy1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "wordy1\n",
      "600\n",
      "['प्रिय', 'स्थानीय', 'समाचार', 'पत्र', 'मुझे', 'लगता', 'कंप्यूटर', 'लोगों', 'पड़ने', 'प्रभाव', 'महान', 'शिक्षण', 'कौशल', 'प्रभावित', 'क्योंकि', 'हमें', 'दोस्तों', 'नए', 'लोगों', 'चैट', 'देते', 'हमें', 'ग्लोब', 'खगोल', 'विज्ञान', 'बारे', 'जानने', 'मदद', 'हमें', 'रोकते', 'बारे', 'आपको', 'लगता', 'किशोर', 'हमेशा', 'दोस्तों', 'फोन', 'रहता', 'आपको', 'कैसा', 'लगेगा', 'चीजों', 'बारे', 'दोस्तों', 'व्यपर', 'साथी', 'चैट', 'खैर', 'कंप्यूटर', 'चैट', 'नया', 'तरीका', 'इंटरनेट', 'सारी', 'साइटें', 'संगठन', 'संगठन', 'फेसबुक', 'माईस्पेस', 'ईत्यादी', 'सोचिए', 'कंप्यूटर', 'बॉस', 'मिलना', 'शुरू', 'किशोर', 'फोन', 'मस्ती', 'क्योंकि', 'इस्तेमाल', 'चाहते', 'आपने', 'देशों', 'राज्यों', 'बारे', 'सीखा', 'वैसे', 'मेरे', 'पास', 'कंप्यूटर', 'इंटरनेट', 'हमारे', 'चल', 'जानने', 'नया', 'तरीका', 'सोच', 'बच्चा', 'कंप्यूटर', 'बिताता', 'उनसे', 'अर्थव्यवस्था', 'समुद्री', 'तल', 'फैलने', 'दिनांक', 'बारे', 'इतना', 'सवाल', 'पूछें', 'जानते', 'आश्चर्यचकित', 'रह', 'जाएंगे', 'कंप्यूटर', 'दिलचस्प', 'कक्षा', 'दिन', 'किताबों', 'पढ़ना', 'बच्चा', 'आपके', 'कंप्यूटर', 'स्थानीय', 'पुस्तकालय', 'दोस्तों', 'फ्रेश', 'तैयार', 'बेहतर', 'सही', 'जानते', 'आपको', 'शायद', 'पता', 'होगा', 'बच्चा', 'अस्पताल', 'बिस्तर', 'ड्राइवबाय', 'कारण', 'मना', 'बच्चे', 'बजाय', 'कंप्यूटर', 'सीखने', 'चैट', 'सिर्फ', 'गेम', 'खेलने', 'सामुदायिक', 'सुरक्षित', 'स्वस्थ', 'रहें', 'मुझे', 'आशा', 'मुझे', 'समझने', 'उससे', 'सहमत', 'बिंदु', 'पहुंच', 'क्योंकि', 'कंप्यूटर', 'आपके', 'बच्चे', 'प्रभाव', 'डाल', 'क्योंकि', 'हमें', 'दोस्तों', 'नए', 'लोगों', 'चैट', 'देता', 'हमें', 'ग्लोब', 'बारे', 'जानने', 'मदद', 'हमें', 'विश्वास', 'रखता', 'रखता', 'सुनने', 'धन्यवाद']\n",
      "\n",
      "wordy3\n",
      "600\n",
      "[['प्रिय', 'स्थानीय', 'समाचार', 'पत्र', 'मुझे', 'लगता', 'कंप्यूटर', 'लोगों', 'पड़ने', 'प्रभाव', 'महान', 'शिक्षण', 'कौशल'], ['प्रभावित', 'क्योंकि', 'हमें', 'दोस्तों'], ['नए', 'लोगों', 'चैट', 'देते', 'हमें', 'ग्लोब', 'खगोल', 'विज्ञान', 'बारे', 'जानने', 'मदद', 'हमें', 'रोकते'], ['आपको', 'लगता'], ['किशोर', 'हमेशा', 'दोस्तों', 'फोन', 'रहता', 'आपको', 'कैसा', 'लगेगा'], ['चीजों', 'बारे', 'दोस्तों', 'व्यपर', 'साथी', 'चैट'], ['खैर', 'कंप्यूटर', 'चैट', 'नया', 'तरीका', 'इंटरनेट', 'सारी', 'साइटें', 'संगठन', 'संगठन', 'फेसबुक', 'माईस्पेस', 'ईत्यादी'], ['सोचिए', 'कंप्यूटर', 'बॉस', 'मिलना', 'शुरू', 'किशोर', 'फोन', 'मस्ती', 'क्योंकि', 'इस्तेमाल', 'चाहते'], ['आपने', 'देशों'], ['राज्यों', 'बारे', 'सीखा'], ['वैसे', 'मेरे', 'पास', 'कंप्यूटर'], ['इंटरनेट', 'हमारे', 'चल', 'जानने', 'नया', 'तरीका'], ['सोच', 'बच्चा', 'कंप्यूटर', 'बिताता', 'उनसे', 'अर्थव्यवस्था', 'समुद्री', 'तल', 'फैलने', 'दिनांक', 'बारे', 'इतना', 'सवाल', 'पूछें', 'जानते'], ['आश्चर्यचकित', 'रह', 'जाएंगे'], ['कंप्यूटर', 'दिलचस्प', 'कक्षा', 'दिन', 'किताबों', 'पढ़ना'], ['बच्चा', 'आपके', 'कंप्यूटर', 'स्थानीय', 'पुस्तकालय', 'दोस्तों', 'फ्रेश', 'तैयार', 'बेहतर', 'सही', 'जानते'], ['आपको', 'शायद', 'पता', 'होगा', 'बच्चा', 'अस्पताल', 'बिस्तर', 'ड्राइवबाय', 'कारण', 'मना'], ['बच्चे', 'बजाय', 'कंप्यूटर', 'सीखने', 'चैट', 'सिर्फ', 'गेम', 'खेलने', 'सामुदायिक', 'सुरक्षित', 'स्वस्थ', 'रहें'], ['मुझे', 'आशा', 'मुझे', 'समझने', 'उससे', 'सहमत', 'बिंदु', 'पहुंच', 'क्योंकि', 'कंप्यूटर', 'आपके', 'बच्चे', 'प्रभाव', 'डाल', 'क्योंकि', 'हमें', 'दोस्तों'], ['नए', 'लोगों', 'चैट', 'देता', 'हमें', 'ग्लोब', 'बारे', 'जानने', 'मदद', 'हमें', 'विश्वास', 'रखता', 'रखता'], ['सुनने', 'धन्यवाद']]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_essay))\n",
    "print('wordy1')\n",
    "print(len(wordy1))\n",
    "print(wordy1[0])\n",
    "print('\\nwordy3')\n",
    "print(len(wordy3))\n",
    "print(wordy3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    #print(len(words))\n",
    "    index2word_set = set(model.wv.index2word)#contains all important the words of dataset\n",
    "    for mword in words:\n",
    "        if mword in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[mword]) \n",
    "    #print(num_words)\n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=42, shuffle=True)\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chotu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Chotu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04073243  0.02042876  0.12234924  0.05511399  0.0834847   0.06172904\n",
      "  0.03367629  0.00716771  0.005562   -0.02836733 -0.01794094  0.04231445\n",
      "  0.06338049 -0.03338175  0.05095775 -0.04568664  0.05929884 -0.05703808\n",
      " -0.03766166  0.02719336  0.05416233  0.04580848 -0.02766491  0.02713832\n",
      "  0.04999499 -0.020891   -0.00244502  0.03430922  0.00171622  0.06397095\n",
      "  0.10243507 -0.03782377  0.0977569   0.04352505 -0.07548579  0.08360419\n",
      "  0.00867173 -0.06701899 -0.01120926  0.08486607 -0.01969915 -0.05378792\n",
      " -0.08276778  0.108992   -0.07679977  0.07893147  0.05301336  0.00932922\n",
      "  0.00741024  0.0354628  -0.05071997  0.12840542  0.06956049 -0.00194527\n",
      "  0.05465797 -0.01131261  0.01831352 -0.02111728  0.01201671  0.04820081\n",
      " -0.0465324   0.00083661 -0.0064483   0.03488184  0.09773426  0.08216082\n",
      "  0.06889141 -0.03977772 -0.02090098  0.07121605  0.06087074 -0.00236995\n",
      "  0.00995281 -0.04624975 -0.10225923 -0.01818929  0.09366198  0.00922456\n",
      " -0.03038746  0.05677593 -0.05033971 -0.08234379 -0.16354984 -0.03791115\n",
      "  0.06057761  0.04889643  0.03409623  0.0146079   0.01097178  0.04186173\n",
      " -0.00135625 -0.04734476 -0.07942324  0.17222165 -0.12796755  0.04862107\n",
      " -0.00415696  0.02210242 -0.04382818 -0.13857768 -0.05439366 -0.02129314\n",
      "  0.02578151 -0.00080789  0.05795748 -0.06664882 -0.0437405  -0.03418694\n",
      "  0.0939     -0.01344442  0.02414024  0.07709394  0.02578327  0.05135545\n",
      "  0.04741671 -0.01493693  0.03342062 -0.05012873  0.12927602  0.05082143\n",
      "  0.11263531  0.02776317 -0.01343445 -0.02310428 -0.01473276 -0.01973195\n",
      "  0.00997769 -0.0072105   0.02312949  0.1090089  -0.02770941 -0.06185448\n",
      " -0.02839155  0.08068199 -0.00992509 -0.05192749  0.00197207  0.07473469\n",
      "  0.05148148 -0.02095053 -0.05042153 -0.04729037 -0.0528125  -0.04254762\n",
      "  0.09973998 -0.08240816  0.08291731  0.07196213  0.00405519 -0.028559\n",
      "  0.01196264  0.03830954 -0.01633334 -0.05756719 -0.07466655 -0.0446237\n",
      "  0.05061319 -0.0573874  -0.03922048  0.03466022 -0.04539159 -0.0262341\n",
      " -0.08187654  0.02932383 -0.02068011 -0.04616949 -0.06663719  0.02270691\n",
      " -0.00535452 -0.10023583  0.03921408 -0.07224035  0.03399863 -0.03095844\n",
      "  0.07661727  0.11622151  0.03040582  0.02337896 -0.08811636  0.00846595\n",
      " -0.06483791 -0.05756726 -0.00239961  0.01794378 -0.05056847 -0.10405353\n",
      "  0.0090158  -0.18528825 -0.04416955 -0.00275984 -0.00499266  0.01015278\n",
      " -0.04268035 -0.00132495  0.00390917  0.06521649 -0.12267779 -0.05417082\n",
      " -0.05554963 -0.02419902  0.0004395  -0.01400978  0.03372025  0.0013156\n",
      " -0.00527075 -0.05971033 -0.04510672  0.07704294  0.00678704 -0.01679382\n",
      "  0.04179491 -0.00210619  0.05047406  0.05789845  0.05355455 -0.11733972\n",
      " -0.04993961  0.01735859 -0.07428976  0.02757997 -0.10355109  0.01620674\n",
      " -0.0626421   0.09210856  0.00106645 -0.02764689 -0.01337037 -0.07451079\n",
      " -0.01093786  0.05414413  0.01702821  0.02229995 -0.04149074 -0.0945956\n",
      " -0.02397446  0.015869    0.01712682 -0.08727033 -0.013386    0.1717965\n",
      "  0.0483134  -0.00585939 -0.00462882  0.07726786 -0.04589086  0.0139171\n",
      " -0.02552443 -0.06106389  0.0591661   0.140013   -0.02226516  0.04036812\n",
      "  0.01893499  0.01405689 -0.05319883  0.01876911  0.06200976 -0.02868602\n",
      "  0.02037996  0.02080528  0.00631369  0.05332541  0.00703699  0.05905684\n",
      " -0.02800908  0.05677579  0.01851712 -0.05377386  0.04896236  0.07448259\n",
      "  0.07810029  0.08375771 -0.00118381 -0.06895266  0.0505489   0.00787921\n",
      " -0.02020875 -0.06373303  0.01928444 -0.05682131 -0.05815526 -0.05035704\n",
      " -0.04309761 -0.08994141 -0.12649974  0.02158338  0.1046994   0.03872613\n",
      "  0.06073656  0.01320357 -0.00415589 -0.00371979  0.10348727  0.09906872\n",
      " -0.03091712  0.07425297  0.01597976 -0.00112379 -0.0266291   0.03577453]\n",
      "(300, 300)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 9.3410 - mae: 2.3561\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 5.3218 - mae: 1.9252\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 4.6616 - mae: 1.7618\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 4.1064 - mae: 1.6207\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 3.8128 - mae: 1.5626\n",
      "Epoch 6/50\n",
      "104/300 [=========>....................] - ETA: 0s - loss: 3.3530 - mae: 1.4639"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0416dfa1001f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mlstm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataVecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;31m#lstm_model.load_weights('C:/Users/Chotu/Videos/final_lstm.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0my_tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataVecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import sklearn\n",
    "from sklearn import model_selection\n",
    "#from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cv = KFold( n_splits=2, shuffle=True, random_state=42)\n",
    "print(cv)\n",
    "results = []\n",
    "result1=[]\n",
    "y_pred = []\n",
    "y_tr=[]\n",
    "X=train_essay\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    #print(traincv, testcv)\n",
    "    \n",
    "    y_test, y_train = y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays =[]\n",
    "    test_essays = []\n",
    "    sentences=[]\n",
    "    for i in traincv:\n",
    "        train_essays.append(wordy1[i])\n",
    "        sentences+=wordy3[i]\n",
    "    \n",
    "    for i in testcv:\n",
    "        test_essays.append(wordy1[i])\n",
    "    #print(len(sentences))\n",
    "    \n",
    "    num_features = 300 \n",
    "    min_word_count = 10\n",
    "    num_workers = 4\n",
    "    context = 16\n",
    "    downsampling = 1e-1\n",
    "    \n",
    "    \n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model=Word2Vec(sentences,workers=num_workers,size=num_features,min_count=min_word_count,window=context,sample=downsampling,sg=1)\n",
    "\n",
    "    #print(model)\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "    #print(len(model.wv.index2word))\n",
    "    print(model['समाचार'])\n",
    "    trainDataVecs = getAvgFeatureVecs(train_essays,model,num_features)\n",
    "    testDataVecs = getAvgFeatureVecs(test_essays ,model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    #print(len(trainDataVecs))\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    print(trainDataVecs.shape)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    #print(trainDataVecs)\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=8, epochs=50)\n",
    "    #lstm_model.load_weights('C:/Users/Chotu/Videos/final_lstm.h5')\n",
    "    y_tr=lstm_model.predict(trainDataVecs)\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    #if count == 5:\n",
    "     #    lstm_model.save('C:/Users/Chotu/Videos/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_tr = np.around(y_tr)\n",
    "    y_pred = np.around(y_pred)\n",
    "    #print((y_pred))\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
    "    result1 = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result1))\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
